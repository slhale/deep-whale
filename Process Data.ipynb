{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems relevant: https://gist.github.com/arunaugustine/5551446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aifc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_folder = \"small_data_sample/right_whale/\"\n",
    "small_file_1 = \"train12.aiff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_aifc_1 = aifc.open(small_folder + small_file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_aifc_1.getnchannels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_aifc_1.getframerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_aifc_1.getnframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = small_aifc_1.readframes(small_aifc_1.getnframes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.fromstring(frames, np.short).byteswap()\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array = np.fromstring(frames, 'float64').byteswap()\n",
    "#array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.fft.rfft(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_files = [\n",
    "    \"small_data_sample/right_whale/train12.aiff\",\n",
    "    \"small_data_sample/right_whale/train28.aiff\",\n",
    "    \"small_data_sample/right_whale/train6.aiff\",\n",
    "    \"small_data_sample/right_whale/train7.aiff\",\n",
    "    \"small_data_sample/right_whale/train9.aiff\"\n",
    "]\n",
    "no_whale_files = [\n",
    "    \"small_data_sample/no_right_whale/train1.aiff\",\n",
    "    \"small_data_sample/no_right_whale/train2.aiff\",\n",
    "    \"small_data_sample/no_right_whale/train3.aiff\",\n",
    "    \"small_data_sample/no_right_whale/train4.aiff\",\n",
    "    \"small_data_sample/no_right_whale/train5.aiff\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_aifcs = []\n",
    "whale_data = []\n",
    "for file_name in whale_files:\n",
    "    whale_aifcs.append(aifc.open(file_name))\n",
    "for aifc_file in whale_aifcs:\n",
    "    frames = aifc_file.readframes(aifc_file.getnframes())\n",
    "    whale_data.append(np.fromstring(frames, np.short).byteswap())\n",
    "whale_data = np.array(whale_data)\n",
    "\n",
    "no_whale_aifcs = []\n",
    "no_whale_data = []\n",
    "for file_name in no_whale_files:\n",
    "    no_whale_aifcs.append(aifc.open(file_name))\n",
    "for aifc_file in no_whale_aifcs:\n",
    "    frames = aifc_file.readframes(aifc_file.getnframes())\n",
    "    no_whale_data.append(np.fromstring(frames, np.short).byteswap())\n",
    "no_whale_data = np.array(no_whale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_data, no_whale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for array in whale_data:\n",
    "    plt.plot(np.fft.rfft(array), linewidth=0.5)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Whale data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for array in no_whale_data:\n",
    "    plt.plot(np.fft.rfft(array), linewidth=0.5)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('No Whale data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not entirely clear where the whale frequency content is, so let's not downsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files_folder = \"/home/sarah/github/deep-whale/data/train/\"\n",
    "#training_aiff_filenames = os.listdir(training_files_folder)\n",
    "label_csv_filename = \"/home/sarah/github/deep-whale/data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels csv\n",
    "labels_csv = np.genfromtxt(label_csv_filename, dtype=None, delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training files as numpy arrays\n",
    "training_numpy_arrays = []\n",
    "training_labels = []\n",
    "#for filename in training_aiff_filenames:\n",
    "for filename_tuple in labels_csv:\n",
    "    filename = filename_tuple[0]\n",
    "    label = filename_tuple[1]\n",
    "    if label == 0:\n",
    "        training_labels.append([0,1])\n",
    "    else:\n",
    "        training_labels.append([1,0])\n",
    "    aifc_file = aifc.open(training_files_folder + filename)\n",
    "    frames = aifc_file.readframes(aifc_file.getnframes())\n",
    "    training_numpy_arrays.append(np.fromstring(frames, np.short).byteswap())\n",
    "training_numpy_arrays = np.array(training_numpy_arrays)\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels, training_numpy_arrays, training_numpy_arrays.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Numpy Timeseries Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the data\n",
    "all_data_file = h5py.File(\"all_whale_training_30000samples_shape4000.hdf5\", 'w-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_file.create_dataset(\"data\", dtype=np.short, shape=training_numpy_arrays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_file['data'][...] = training_numpy_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_file.create_dataset(\"labels\", dtype='i', shape=training_labels.shape)\n",
    "all_data_file['labels'][...] = training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_file.flush()\n",
    "all_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_part = np.fft.rfft(training_numpy_arrays[0]).real\n",
    "imag_part = np.fft.rfft(training_numpy_arrays[0]).imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(real_part, label='real')\n",
    "plt.plot(imag_part, label='imag')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_parts = np.concatenate([real_part, imag_part])\n",
    "plt.figure()\n",
    "plt.plot(concat_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof Z says to NOT append. Just do two different channels. (She said \"like tuples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_imag_array = np.array([real_part, imag_part])\n",
    "real_imag_array, real_imag_array.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_numpy_ffts = []\n",
    "for i in range(len(training_numpy_arrays)):\n",
    "    fft = np.fft.rfft(training_numpy_arrays[i])\n",
    "    real_part = fft.real\n",
    "    imag_part = fft.imag\n",
    "    training_numpy_ffts.append(np.array([real_part, imag_part]).T)\n",
    "training_numpy_ffts = np.array(training_numpy_ffts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_numpy_ffts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Numpy FFT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fft_data_file = h5py.File(\"all_whale_training_fft_30000samples_shape2001x2.hdf5\", 'w-')\n",
    "\n",
    "all_fft_data_file.create_dataset(\"data\", dtype=np.short, shape=training_numpy_ffts.shape)\n",
    "all_fft_data_file['data'][...] = training_numpy_ffts\n",
    "\n",
    "all_fft_data_file.create_dataset(\"labels\", dtype='i', shape=training_labels.shape)\n",
    "all_fft_data_file['labels'][...] = training_labels\n",
    "\n",
    "all_fft_data_file.flush()\n",
    "all_fft_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.2\n",
    "(1-test_frac) * 30000, test_frac * 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frac = 0.2\n",
    "(1-val_frac) * (24000.0), val_frac * (24000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aifc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fft_data_file = h5py.File(\"all_whale_training_fft_30000samples_shape2001x2.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fft_data_file['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = []\n",
    "for i in range(len(all_fft_data_file['labels'])):\n",
    "    if all_fft_data_file['labels'][i][0] == 1:\n",
    "        flat_labels.append(1)\n",
    "    else:\n",
    "        flat_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(flat_labels, '-', linewidth=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems... close enough to randomly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indicies = int(0.2 * 30000)\n",
    "train_indicies = test_indicies + int(0.8 * (0.8 * 30000))\n",
    "validataion_indicies = train_indicies + int(0.8 * (0.2 * 30000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indicies, train_indicies, validataion_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split up the data!\n",
    "\n",
    "# Test data\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for i in range(test_indicies):\n",
    "    test_data.append(all_fft_data_file['data'][i])\n",
    "    test_labels.append(all_fft_data_file['labels'][i])\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Training data \n",
    "training_data = []\n",
    "training_labels = []\n",
    "for i in range(test_indicies, train_indicies):\n",
    "    training_data.append(all_fft_data_file['data'][i])\n",
    "    training_labels.append(all_fft_data_file['labels'][i])\n",
    "training_data = np.array(training_data)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "# Validation data \n",
    "validation_data = []\n",
    "validation_labels = []\n",
    "for i in range(train_indicies, validataion_indicies):\n",
    "    validation_data.append(all_fft_data_file['data'][i])\n",
    "    validation_labels.append(all_fft_data_file['labels'][i])\n",
    "validation_data = np.array(validation_data)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, training_data.shape, validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split data\n",
    "\n",
    "split_fft_data_file = h5py.File(\"whale_training_fft_samples19200train4800val6000test_shape2001x2.hdf5\", 'w-')\n",
    "\n",
    "split_fft_data_file.create_dataset(\"training_data\", dtype=np.short, shape=training_data.shape)\n",
    "split_fft_data_file['training_data'][...] = training_data\n",
    "\n",
    "split_fft_data_file.create_dataset(\"training_labels\", dtype='i', shape=training_labels.shape)\n",
    "split_fft_data_file['training_labels'][...] = training_labels\n",
    "\n",
    "split_fft_data_file.create_dataset(\"validation_data\", dtype=np.short, shape=validation_data.shape)\n",
    "split_fft_data_file['validation_data'][...] = validation_data\n",
    "\n",
    "split_fft_data_file.create_dataset(\"validation_labels\", dtype='i', shape=validation_labels.shape)\n",
    "split_fft_data_file['validation_labels'][...] = validation_labels\n",
    "\n",
    "split_fft_data_file.create_dataset(\"testing_data\", dtype=np.short, shape=test_data.shape)\n",
    "split_fft_data_file['testing_data'][...] = test_data\n",
    "\n",
    "split_fft_data_file.create_dataset(\"testing_labels\", dtype='i', shape=test_labels.shape)\n",
    "split_fft_data_file['testing_labels'][...] = test_labels\n",
    "\n",
    "split_fft_data_file.flush()\n",
    "split_fft_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
