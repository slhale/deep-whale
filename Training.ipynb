{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'timeseries_feedforward_model.json'\n",
    "\n",
    "# Load the model\n",
    "with open(model_file, 'r') as json_file:\n",
    "        model = keras.models.model_from_json(json_file.read())\n",
    "        \n",
    "# Compile the model\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'whale_training_samples19200train4800val6000test_shape4000.hdf5'\n",
    "\n",
    "# Define normalizer for loading data\n",
    "def normalize_long_axis(x):\n",
    "    # Assuming data format (num_samples, time_series, channels)\n",
    "    # and we want to normalize over the time series\n",
    "    return keras.utils.normalize(x, axis=1)\n",
    "\n",
    "# Load training and validataion data\n",
    "# Data is normalized upon loading\n",
    "training_data = keras.utils.io_utils.HDF5Matrix(\n",
    "        data_file, 'training_data', normalizer=normalize_long_axis)\n",
    "validation_data = keras.utils.io_utils.HDF5Matrix(\n",
    "        data_file, 'validation_data', normalizer=normalize_long_axis)\n",
    "training_labels = keras.utils.io_utils.HDF5Matrix(\n",
    "        data_file, 'training_labels')\n",
    "validation_labels = keras.utils.io_utils.HDF5Matrix(\n",
    "        data_file, 'validation_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = []\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "# Save training log\n",
    "callbacks.append(keras.callbacks.CSVLogger(\"training_log_\"+time.strftime(\"%Y%m%d\")+\"_\"\n",
    "                                           +str(batch_size)+\"batch_\"\n",
    "                                           +str(epochs)+\"epochs_\"\n",
    "                                           +model_file[:-5]+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(training_data, training_labels, epochs=epochs,\n",
    "          batch_size=batch_size, verbose=2, shuffle='batch',\n",
    "          validation_data=(validation_data, validation_labels),\n",
    "          callbacks=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
